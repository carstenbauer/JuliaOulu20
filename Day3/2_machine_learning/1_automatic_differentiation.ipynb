{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know how to take derivatives.\n",
    "```julia\n",
    "f(x) = 5*x^2 + 3\n",
    "\n",
    "df(x) = 10*x\n",
    "\n",
    "ddf(x) = 10\n",
    "```\n",
    "\n",
    "The promise of AD is\n",
    "\n",
    "```julia\n",
    "df(x) = derivative(f, x)\n",
    "\n",
    "ddf(x) = derivative(df, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What AD is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic differentiation:**\n",
    "$$ \\frac{d}{dx}x^n = n x^{n-1}. $$\n",
    "\n",
    "**Numerical differentiation:**\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{\\Delta h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to AD is the application of the chain rule\n",
    "$$\\dfrac{d}{dx} f(g(x)) = \\dfrac{df}{dg} \\dfrac{dg}{dx}$$\n",
    "\n",
    "Consider the function $f(a,b) = \\ln(ab + \\max(a,2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) = log(a*b + sin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.1\n",
    "b = 2.4\n",
    "f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the function into the elementary steps, it corresponds to the following \"*computational graph*\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph(a,b)\n",
    "    c1 = a*b\n",
    "    c2 = sin(a)\n",
    "    c3 = c1 + c2\n",
    "    c4 = log(c3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) == f_graph(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\frac{\\partial f}{\\partial a}$ we have to apply the chain rule multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial f}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial a}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial a} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial a}\\right)  \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph_derivative(a,b)\n",
    "    c1 = a*b\n",
    "    c1_ϵ = b\n",
    "    \n",
    "    c2 = sin(a)\n",
    "    c2_ϵ = cos(a)\n",
    "    \n",
    "    c3 = c1 + c2\n",
    "    c3_ϵ = c1_ϵ + c2_ϵ\n",
    "    \n",
    "    c4 = log(c3)\n",
    "    c4_ϵ = 1/c3 * c3_ϵ\n",
    "    \n",
    "    c4, c4_ϵ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_graph_derivative(a,b)[2] == f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we automate this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D for \"dual number\", invented by Clifford in 1873.\n",
    "struct D <: Number\n",
    "    x::Float64 # value\n",
    "    ϵ::Float64 # derivative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, *, /, -, sin, log, convert, promote_rule\n",
    "\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ) # sum rule\n",
    "a::D - b::D = D(a.x - b.x, a.ϵ - b.ϵ)\n",
    "a::D * b::D = D(a.x * b.x, a.x * b.ϵ + a.ϵ * b.x) # product rule\n",
    "a::D / b::D = D(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ)/b.x^2) # quotient rule\n",
    "sin(a::D) = D(sin(a.x), cos(a.x) * a.ϵ)\n",
    "log(a::D) = D(log(a.x), 1/a.x * a.ϵ)\n",
    "\n",
    "Base.convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "Base.promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! That was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b).ϵ ≈ f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this work?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick of forward mode AD is to make the computer do the rewrite `f -> f_graph_derivative` for you.\n",
    "\n",
    "Importantly, the compiler sees the entire \"rewritten\" code and can therefore apply all kinds of optimizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f_graph_derivative(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for our small forward AD\n",
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->f(x,b), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->3*x^2+4x+5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->sin(x)*log(x), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = derivative(a->f(a,b),x) # partial derivative wrt a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking derivatives of code: Babylonian sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat $t \\leftarrow (t + x/2)/2$ until $t$ converges to $\\sqrt{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10)\n",
    "    t = (1+x)/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "    end\n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(2), √2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "xs = 0:0.01:49\n",
    "\n",
    "p = plot(title = \"Those Babylonians really knew what they were doing\")\n",
    "for i in 1:5\n",
    "    plot!(p, xs, [Babylonian(x; N=i) for x in xs], label=\"Iteration $i\")\n",
    "end\n",
    "\n",
    "plot!(p, xs, sqrt.(xs), label=\"sqrt\", color=:black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and now the derivative, automagically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same babylonian algorithm with no rewrite at all computes properly the derivative as the check shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "√5, 0.5 / √5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It just works and is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion? Works as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(x, n) = n <= 0 ? 1 : x*pow(x, n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x -> pow(x,3), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically (because we can)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is mathematically equivalent, though not what the computation is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "display(\"Iterations as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(Babylonian(x; N=k)))\n",
    "end\n",
    "\n",
    "display(\"Derivatives as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(diff(simplify(Babylonian(x; N=k)), x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood how forward AD works, we can use the more feature complete package [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial x}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial x} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial x}\\right)  \\right)$\n",
    "\n",
    "Reverse mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\left( \\dfrac{\\partial f}{\\partial c_3}\\dfrac{\\partial c_3}{\\partial c_4}   \\right) \\dfrac{\\partial c_4}{\\partial x} = \\left( \\left( \\dfrac{\\partial f}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial c_3} + \\dfrac{\\partial f}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial c_3} \\right) \\dfrac{\\partial c_3}{\\partial c_4} \\right) \\dfrac{\\partial c_4}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode AD requires $n$ passes in order to compute an $n$-dimensional\n",
    "gradient.\n",
    "\n",
    "Reverse mode AD requires only a single run in order to compute a complete gradient but requires two passes through the graph: a forward pass during which necessary intermediate values are computed and a backward pass which computes the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rule of thumb:*\n",
    "\n",
    "Forward mode is good for $\\mathbb{R} \\rightarrow \\mathbb{R}^n$ while reverse mode is good for $\\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient source-to-source reverse mode AD is implemented in [Zygote.jl](https://github.com/FluxML/Zygote.jl), the AD underlying [Flux.jl](https://fluxml.ai/) (since version 0.10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 5*x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(f, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none gradient(f,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some nice reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog posts:\n",
    "\n",
    "* ML in Julia: https://julialang.org/blog/2018/12/ml-language-compiler\n",
    "\n",
    "* Nice example: https://fluxml.ai/2019/03/05/dp-vs-rl.html\n",
    "\n",
    "* Nice interactive examples: https://fluxml.ai/experiments/\n",
    "\n",
    "* Why Julia for ML? https://julialang.org/blog/2017/12/ml&pl\n",
    "\n",
    "* Neural networks with differential equation layers: https://julialang.org/blog/2019/01/fluxdiffeq\n",
    "\n",
    "* Implement Your Own Automatic Differentiation with Julia in ONE day : http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/\n",
    "\n",
    "* Implement Your Own Source To Source AD in ONE day!: http://blog.rogerluo.me/2019/07/27/yassad/\n",
    "\n",
    "Repositories:\n",
    "\n",
    "* AD flavors, like forward and reverse mode AD: https://github.com/MikeInnes/diff-zoo (Mike is one of the smartest Julia ML heads)\n",
    "\n",
    "Talks:\n",
    "\n",
    "* AD is a compiler problem: https://juliacomputing.com/assets/pdf/CGO_C4ML_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
